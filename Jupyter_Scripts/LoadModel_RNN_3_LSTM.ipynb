{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODELING LIBRARIES #####\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "# from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from patsy import dmatrices, dmatrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "##### FORMATTING AND GRAPHING LIBRARIES #####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##### TIMING AND UTILITY LIBRARIES #####\n",
    "import sqlalchemy as db\n",
    "from datetime import datetime\n",
    "import timeit\n",
    "from dateutil.relativedelta import relativedelta \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pre-built dataset...\n",
      "Dropping 1 null values...\n"
     ]
    }
   ],
   "source": [
    "##### IMPORT PREPARED DATA #####\n",
    "print('Reading pre-built dataset...')\n",
    "df_load = pd.read_csv('../PJM_Weekly_Model/sample_base_data.csv', index_col = 0, parse_dates = [0])\n",
    "\n",
    "# Drop all lag columns for us in RNN\n",
    "lag_sq_cols = [column for column in df_load.columns if 'Lag' in column or 'Sq' in column]\n",
    "df_load = df_load.drop(columns = lag_sq_cols)\n",
    "\n",
    "# Drop null rows - should only be losing daylight savings in March\n",
    "dropped = df_load.shape[0] - df_load.dropna().shape[0]\n",
    "print('Dropping %s null values...'%(dropped))\n",
    "df_load = df_load.dropna()\n",
    "\n",
    "# Convert date, time, holiday columns to categorical variables\n",
    "for col in ['Month','WeekDay','Day','Hour']:\n",
    "    df_load[col] = df_load[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defining testing and training set...\n"
     ]
    }
   ],
   "source": [
    "##### FINAL DATA PREPARATION #####\n",
    "print(\"\\nDefining testing and training set...\") \n",
    "\n",
    "# Set random seed\n",
    "random.seed(238)\n",
    "\n",
    "# Use Patsy to create the one-hot encoded dummy variables with interactions \n",
    "y, X =  dmatrices('value~Light+WWP+THI+Month+Day+WeekDay+Hour+Holiday',df_load,return_type='dataframe')\n",
    "\n",
    "# Split data into training and testing data sets with two-year training sample (8760 h/yr * 2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 17520 / X.shape[0], shuffle = False)\n",
    "\n",
    "# Standardize both datasets - create fit to use on backcast dataset\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training sequences\n",
    "train_sequences = TimeseriesGenerator(X_train_ss, y_train['value'], length = 48, batch_size = 512)\n",
    "\n",
    "# Create test sequences\n",
    "test_sequences = TimeseriesGenerator(X_test_ss, y_test['value'], length = 48, batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 48, 75)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building and training model...\n",
      "Epoch 1/10\n",
      "35/35 [==============================] - 14s 414ms/step - loss: 8121101312.0000 - mean_absolute_percentage_error: 97.8967 - val_loss: 12585256960.0000 - val_mean_absolute_percentage_error: 73.6305\n",
      "Epoch 2/10\n",
      "35/35 [==============================] - 14s 412ms/step - loss: 243339182080.0000 - mean_absolute_percentage_error: 209.6855 - val_loss: 65850105856.0000 - val_mean_absolute_percentage_error: 127.2924\n",
      "Epoch 3/10\n",
      "35/35 [==============================] - 15s 417ms/step - loss: 8480578048.0000 - mean_absolute_percentage_error: 100.1651 - val_loss: 8347369984.0000 - val_mean_absolute_percentage_error: 99.9964\n",
      "Epoch 4/10\n",
      "35/35 [==============================] - 14s 408ms/step - loss: 8398778880.0000 - mean_absolute_percentage_error: 99.9960 - val_loss: 8346988544.0000 - val_mean_absolute_percentage_error: 99.9938\n",
      "Epoch 5/10\n",
      "35/35 [==============================] - 15s 414ms/step - loss: 8128927232.0000 - mean_absolute_percentage_error: 94.3617 - val_loss: 6724927488.0000 - val_mean_absolute_percentage_error: 81.2397\n",
      "Epoch 6/10\n",
      "35/35 [==============================] - 15s 416ms/step - loss: 8145768448.0000 - mean_absolute_percentage_error: 97.3571 - val_loss: 8347213312.0000 - val_mean_absolute_percentage_error: 99.9953\n",
      "Epoch 7/10\n",
      "35/35 [==============================] - 14s 411ms/step - loss: 8398903808.0000 - mean_absolute_percentage_error: 99.9968 - val_loss: 8347337216.0000 - val_mean_absolute_percentage_error: 99.9959\n",
      "Epoch 8/10\n",
      "35/35 [==============================] - 14s 412ms/step - loss: 8398898688.0000 - mean_absolute_percentage_error: 99.9967 - val_loss: 8347379712.0000 - val_mean_absolute_percentage_error: 99.9963\n",
      "Epoch 9/10\n",
      "35/35 [==============================] - 14s 412ms/step - loss: 8398867968.0000 - mean_absolute_percentage_error: 99.9966 - val_loss: 8347311104.0000 - val_mean_absolute_percentage_error: 99.9958\n",
      "Epoch 10/10\n",
      "35/35 [==============================] - 15s 416ms/step - loss: 8398815232.0000 - mean_absolute_percentage_error: 99.9963 - val_loss: 8347187200.0000 - val_mean_absolute_percentage_error: 99.9948\n",
      "RNN fit created in 150.40 seconds\n"
     ]
    }
   ],
   "source": [
    "##### RNN MODEL #####\n",
    "tic = timeit.default_timer()\n",
    "print(\"\\nBuilding and training model...\") \n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape = (48,75), return_sequences = True, activation = 'relu'))\n",
    "model.add(LSTM(64, return_sequences = True, activation = 'relu'))\n",
    "model.add(LSTM(32, return_sequences = True, activation = 'relu'))\n",
    "model.add(LSTM(16, return_sequences = False, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate = 0.001), loss = 'mean_squared_error', metrics = 'mean_absolute_percentage_error')\n",
    "\n",
    "history = model.fit(train_sequences, validation_data = test_sequences, epochs = 10)\n",
    "\n",
    "toc = timeit.default_timer()\n",
    "print('RNN fit created in %0.2f seconds' % (toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label = 'Train Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Test Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['mean_absolute_percentage_error'], label = 'Train MAPE')\n",
    "plt.plot(history.history['val_mean_absolute_percentage_error'], label = 'Test MAPE')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['predicted'] = np.append([np.NaN] * 48, model.predict(train_sequences).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test['predicted'] = np.append([np.NaN] * 48, model.predict(test_sequences).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_train)\n",
    "plt.legend(y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test['2019-06'])\n",
    "plt.legend(y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test['20190612'])\n",
    "plt.legend(y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
