{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODELING LIBRARIES #####\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "# from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from patsy import dmatrices, dmatrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "##### FORMATTING AND GRAPHING LIBRARIES #####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##### TIMING AND UTILITY LIBRARIES #####\n",
    "import sqlalchemy as db\n",
    "from datetime import datetime\n",
    "import timeit\n",
    "from dateutil.relativedelta import relativedelta \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IMPORT PREPARED DATA #####\n",
    "print('Reading pre-built dataset...')\n",
    "df_load = pd.read_csv('../PJM_Weekly_Model/sample_base_data.csv', index_col = 0, parse_dates = [0])\n",
    "\n",
    "# Drop all lag columns for us in RNN\n",
    "lag_sq_cols = [column for column in df_load.columns if 'Lag' in column or 'Sq' in column]\n",
    "df_load = df_load.drop(columns = lag_sq_cols)\n",
    "\n",
    "# Drop null rows - should only be losing daylight savings in March\n",
    "dropped = df_load.shape[0] - df_load.dropna().shape[0]\n",
    "print('Dropping %s null values...'%(dropped))\n",
    "df_load = df_load.dropna()\n",
    "\n",
    "# Convert date, time, holiday columns to categorical variables\n",
    "for col in ['Month','WeekDay','Day','Hour']:\n",
    "    df_load[col] = df_load[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### FINAL DATA PREPARATION #####\n",
    "print(\"\\nDefining testing and training set...\") \n",
    "\n",
    "# Set random seed\n",
    "random.seed(238)\n",
    "\n",
    "# Use Patsy to create the one-hot encoded dummy variables with interactions \n",
    "y, X =  dmatrices('value~Light+WWP+THI+Month+Day+WeekDay+Hour+Holiday',df_load,return_type='dataframe')\n",
    "\n",
    "# Split data into training and testing data sets with two-year training sample (8760 h/yr * 2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 17520 / X.shape[0], shuffle = False)\n",
    "\n",
    "# Standardize both datasets - create fit to use on backcast dataset\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def model_fn(seq_X_train = X_train_ss, seq_y_train = y_train['value'],\n",
    "             seq_X_test = X_test_ss, seq_y_test = y_test['value'],\n",
    "             seq_length = 48, seq_batch_size = 512,\n",
    "             layer_one_neurons = 128, layer_one_dropout = 0,\n",
    "             layer_two_neurons = 64, layer_two_dropout = 0,\n",
    "             layer_three_neurons = 32, layer_three_dropout = 0,\n",
    "             layer_four_neurons = 16, layer_four_dropout = 0,)\n",
    "    print(\"\\nBuilding and training model...\") \n",
    "    # Create training sequences\n",
    "    train_sequences = TimeseriesGenerator(seq_X_train, seq_y_train,\n",
    "                                          length = seq_length,\n",
    "                                          batch_size = seq_batch_size)\n",
    "\n",
    "    # Create test sequences\n",
    "    test_sequences = TimeseriesGenerator(seq_X_test, seq_y_test\n",
    "                                         , length = seq_length\n",
    "                                         , batch_size = seq_batch_size)\n",
    "\n",
    "    ##### RNN MODEL #####\n",
    "    tic = timeit.default_timer()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape = (48,75), return_sequences = True, activation = 'relu', ))\n",
    "    model.add(Dropout(layer_one_dropout))\n",
    "    model.add(GRU(64, return_sequences = True, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(32, return_sequences = True, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(16, return_sequences = False, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "    model.compile(optimizer = Adam(learning_rate = 0.001),\n",
    "                  loss = 'mean_squared_error',\n",
    "                  metrics = 'mean_absolute_percentage_error')\n",
    "\n",
    "    history = model.fit(train_sequences, validation_data = test_sequences, epochs = 40)\n",
    "\n",
    "    toc = timeit.default_timer()\n",
    "    print('RNN fit created in %0.2f seconds' % (toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label = 'Train Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Test Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['mean_absolute_percentage_error'], label = 'Train MAPE')\n",
    "plt.plot(history.history['val_mean_absolute_percentage_error'], label = 'Test MAPE')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['predicted'] = np.append([np.NaN] * 48, model.predict(train_sequences).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test['predicted'] = np.append([np.NaN] * 48, model.predict(test_sequences).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_train)\n",
    "plt.legend(y_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test['2019-06'])\n",
    "plt.legend(y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test['20190612'])\n",
    "plt.legend(y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
